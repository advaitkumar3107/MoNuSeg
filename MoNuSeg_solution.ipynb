{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoNuSeg_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advaitkumar3107/MoNuSeg/blob/master/MoNuSeg_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm6tMt9DKWbl",
        "colab_type": "text"
      },
      "source": [
        "## **UNET_DETECTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLqvxdFIOwZN",
        "colab_type": "code",
        "outputId": "6d7aec5f-01f9-4fbe-a644-3f18c03bbcb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "os.chdir('/content/drive/My Drive/amit sethi')\n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"../../\")\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "# Data Path\n",
        "TEST_PATH = 'Datasets/test/'\n",
        "TRAIN_PATH = 'Datasets/nucleus/'\n",
        "\n",
        "# Get train and test IDs\n",
        "test_ids = next(os.walk(TEST_PATH))[1]\n",
        "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Root directory of the project\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"/content/drive/My Drive/amit sethi/logs\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xhwHpHIRYg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def mean_iou(y_true, y_pred):\n",
        "      prec = []\n",
        "      for t in np.arange(0.5, 1.0, 0.05):\n",
        "          y_pred_ = tf.compat.v1.to_int32(y_pred > t)\n",
        "          score, up_opt = tf.compat.v1.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "          K.get_session().run(tf.local_variables_initializer())\n",
        "          with tf.control_dependencies([up_opt]):\n",
        "              score = tf.identity(score)\n",
        "          prec.append(score)\n",
        "      return K.mean(K.stack(prec), axis=0)\n",
        "    \n",
        "  def get_ax(rows=1, cols=1, size=8):\n",
        "      \"\"\"Return a Matplotlib Axes array to be used in\n",
        "      all visualizations in the notebook. Provide a\n",
        "      central point to control graph sizes.\n",
        "      \n",
        "      Change the default size attribute to control the size\n",
        "      of rendered images\n",
        "      \"\"\"\n",
        "      _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "      return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5rYNzcpUoW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unet_path = os.path.join(MODEL_DIR, \"unet.h5\")\n",
        "mask_rcnn_path = os.path.join(MODEL_DIR, \"mask_rcnn.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg8HFdaJ_Qm8",
        "colab_type": "code",
        "outputId": "ada6f753-f1bf-4c87-ef5b-6f5856067422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "X_test = np.zeros((len(test_ids), 128, 128, 3), dtype=np.uint8)\n",
        "Y_test = np.zeros((len(test_ids), 128, 128, 1), dtype=np.bool)\n",
        "sizes_test = []\n",
        "X_train = np.zeros((len(train_ids), 128, 128, 3), dtype = np.uint8)\n",
        "Y_train = np.zeros((len(train_ids), 128, 128, 1), dtype = np.bool)\n",
        "sizes_train = []\n",
        "print('Getting and resizing test images ... ')\n",
        "sys.stdout.flush()\n",
        "\n",
        "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
        "    path = TEST_PATH + id_\n",
        "    img = imread(path + '/images/' + id_ + '.png')[:,:,:3]\n",
        "    sizes_test.append([img.shape[0], img.shape[1]])\n",
        "    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n",
        "    X_test[n] = img\n",
        "    mask = np.zeros((128, 128, 1), dtype=np.bool)\n",
        "    \n",
        "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
        "        mask_ = imread(path + '/masks/' + mask_file)\n",
        "        mask_ = np.expand_dims(resize(mask_, (128, 128), mode='constant', \n",
        "                                      preserve_range=True), axis=-1)\n",
        "        mask = np.maximum(mask, mask_)\n",
        "    Y_test[n] = mask\n",
        "\n",
        "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
        "    path = TRAIN_PATH + id_\n",
        "    img = imread(path + '/images/' + id_ + '.png')[:,:,:3]\n",
        "    sizes_train.append([img.shape[0], img.shape[1]])\n",
        "    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n",
        "    X_train[n] = img\n",
        "    mask = np.zeros((128, 128, 1), dtype=np.bool)\n",
        "    \n",
        "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
        "        mask_ = imread(path + '/masks/' + mask_file)\n",
        "        mask_ = np.expand_dims(resize(mask_, (128, 128), mode='constant', \n",
        "                                      preserve_range=True), axis=-1)\n",
        "        mask = np.maximum(mask, mask_)\n",
        "    Y_train[n] = mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting and resizing test images ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:09<00:00,  1.60s/it]\n",
            "100%|██████████| 24/24 [00:46<00:00,  2.43s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml2JRX-8_Q-L",
        "colab_type": "code",
        "outputId": "08ea1476-6a2e-424c-ad37-e94f6dde5df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        }
      },
      "source": [
        "unet = load_model(unet_path, custom_objects={'mean_iou': mean_iou})\n",
        "preds_train = unet.predict(X_train, verbose = 1)\n",
        "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "preds_train_upsampled = []\n",
        "for i in range(len(preds_train)):\n",
        "  preds_train_upsampled.append(resize(np.squeeze(preds_train[i]),\n",
        "                                      (sizes_train[i][0], sizes_train[i][1]),\n",
        "                               mode = 'constant', preserve_range = True))\n",
        "  \n",
        "preds_test = unet.predict(X_test, verbose=1)\n",
        "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
        "preds_test_upsampled = []\n",
        "for i in range(len(preds_test)):\n",
        "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
        "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
        "                                       mode='constant', preserve_range=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-2-6005d7915baf>:4: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "24/24 [==============================] - 2s 64ms/step\n",
            "6/6 [==============================] - 0s 50ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dge5jas5_RIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = Y_test*1\n",
        "y_train = Y_train*1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAFGEJH7GceY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def predictions(y_true, y_pred):\n",
        "      corrects = []\n",
        "      length = y_true.shape[0]\n",
        "      height = y_true.shape[1]\n",
        "      width = y_true.shape[2]\n",
        "      total_points = height*width\n",
        "      background_class = np.zeros((height, width))\n",
        "      nucleus_class = np.ones((height, width))\n",
        "\n",
        "      for i in np.arange(length):\n",
        "        correct = np.array(y_true[i] == y_pred[i])\n",
        "        incorrect = total_points - np.sum(correct*1)\n",
        "        true_positive = 0\n",
        "        true_negative = 0\n",
        "        for j in np.arange(height):\n",
        "          for k in np.arange(width):\n",
        "            if (y_true[i][j][k] == y_pred[i][j][k]) and (y_true[i][j][k] == 1):\n",
        "              true_positive = true_positive + 1\n",
        "            elif (y_true[i][j][k] == y_pred[i][j][k]) and (y_true[i][j][k] == 0):\n",
        "              true_negative = true_negative+1\n",
        "        nucleus_iou = true_positive/(true_positive + incorrect)\n",
        "        background_iou = true_negative/(true_negative + incorrect)\n",
        "        mean = nucleus_iou/2 + background_iou/2\n",
        "        corrects.append(mean)\n",
        "      return corrects"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45iBPJ3OviKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_test = predictions(y_test, preds_test_t)\n",
        "score_train = predictions(y_train, preds_train_t)\n",
        "avg_test = np.sum(score_test)/len(score_test)\n",
        "avg_train = np.sum(score_train)/len(score_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0jWx-lzvzRz",
        "colab_type": "code",
        "outputId": "9842b47f-338a-4803-9dd5-cd0228aa9577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(avg_test)\n",
        "print(avg_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7319809977715382\n",
            "0.725936065494391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2sGzjP9KjFU",
        "colab_type": "text"
      },
      "source": [
        "## **MASK_RCNN DETECTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLI9J4VfGcib",
        "colab_type": "code",
        "outputId": "70047e74-faad-49b6-adfb-fc1eae8bddd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "\n",
        "os.chdir('/content/drive/My Drive/amit sethi')\n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"../../\")\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "# Data Path\n",
        "TRAIN_PATH = 'Datasets/nucleus/'\n",
        "TEST_PATH = 'Datasets/test/'\n",
        "\n",
        "# Get train and test IDs\n",
        "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
        "test_ids = next(os.walk(TEST_PATH))[1]\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Root directory of the project\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"/content/drive/My Drive/amit sethi/logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"/content/drive/My Drive/amit sethi/mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ShapesConfig(Config):\n",
        "    \"\"\"Configuration for training on the dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    # Train on 1 GPU and 1 images per GPU. We can put multiple images on each\n",
        "    # GPU. Batch size is (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MAX_INSTANCES = 400\n",
        "    DETECTION_MIN_CONFIDENCE = 0\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # background + nucleus\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "    IMAGE_MIN_SCALE = 2.0\n",
        "    MAX_GT_INSTANCES = 200\n",
        "    MEAN_PIXEL = [47.49, 41.63, 51.28]\n",
        "    POST_NMS_ROIS_INFERENCE = 2000\n",
        "    POST_NMS_ROIS_TRAINING = 1000\n",
        "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
        "\n",
        "    # Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 800\n",
        "\n",
        "    # set number of epoch\n",
        "    STEPS_PER_EPOCH = 200\n",
        "\n",
        "    # set validation steps \n",
        "    VALIDATION_STEPS = 50\n",
        "    \n",
        "class NucleusInferenceConfig(ShapesConfig):\n",
        "    # Set batch size to 1 to run one image at a time\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    # Don't resize imager for inferencing\n",
        "    IMAGE_RESIZE_MODE = \"pad64\"\n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more propsals.\n",
        "    RPN_NMS_THRESHOLD = 0.7\n",
        "config = NucleusInferenceConfig()\n",
        "config.display()\n",
        "\n",
        "\n",
        "class ShapesDataset(utils.Dataset):\n",
        "    \n",
        "    def load_shapes(self, mode):\n",
        "        \n",
        "        # Add classes\n",
        "        self.add_class(\"shapes\", 1, \"nucleus\")\n",
        "        \n",
        "\n",
        "        if mode == \"train\":  \n",
        "            for n, id_ in enumerate(train_ids):\n",
        "                if n < int(len(train_ids) * 0.9):\n",
        "                    path = TRAIN_PATH + id_\n",
        "                    img_path = path + '/images/'\n",
        "                    self.add_image(\"shapes\", image_id=id_, path=img_path)\n",
        "              \n",
        "        if mode == \"val\":   \n",
        "            for n, id_ in enumerate(train_ids):\n",
        "                if n >= int(len(train_ids) * 0.9):\n",
        "                    path = TRAIN_PATH + id_\n",
        "                    img_path = path + '/images/'\n",
        "                    self.add_image(\"shapes\", image_id=id_, path=img_path)      \n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \n",
        "        info = self.image_info[image_id]\n",
        "        info = info.get(\"id\")\n",
        "       \n",
        "        path = TRAIN_PATH + info\n",
        "        img = imread(path + '/images/' + info + '.png')[:,:,:3]\n",
        "        img = resize(img, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n",
        "       \n",
        "        return img\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the shapes data of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"shapes\":\n",
        "            return info[\"shapes\"]\n",
        "        else:\n",
        "            super(self.__class__).image_reference(self, image_id)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
        "        \"\"\"\n",
        "        \n",
        "        info = self.image_info[image_id]\n",
        "        info = info.get(\"id\")\n",
        "        path = TRAIN_PATH + info\n",
        "        number_of_masks = len(next(os.walk(path + '/masks/'))[2])\n",
        "        mask = np.zeros([config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], number_of_masks], dtype=np.uint8)\n",
        "        iterator = 0\n",
        "        for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
        "            mask_ = imread(path + '/masks/' + mask_file)\n",
        "            mask_ = resize(mask_, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n",
        "            mask[:, :, iterator] = mask_\n",
        "            iterator += 1\n",
        "            # Handle occlusions\n",
        "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
        "        for i in range(number_of_masks-2, -1, -1):\n",
        "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
        "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
        "            \n",
        "        # Map class names to class IDs.\n",
        "        class_ids = np.ones((number_of_masks,), dtype=int)\n",
        "        \n",
        "        return mask, class_ids.astype(np.int32)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        400\n",
            "DETECTION_MIN_CONFIDENCE       0\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  128\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  128\n",
            "IMAGE_MIN_SCALE                2.0\n",
            "IMAGE_RESIZE_MODE              pad64\n",
            "IMAGE_SHAPE                    [128 128   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               200\n",
            "MEAN_PIXEL                     [47.49, 41.63, 51.28]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           shapes\n",
            "NUM_CLASSES                    2\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        2000\n",
            "POST_NMS_ROIS_TRAINING         1000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    64\n",
            "STEPS_PER_EPOCH                200\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           800\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DWRmJCEg8Jc",
        "colab_type": "code",
        "outputId": "7f97869f-9349-4630-83f0-b2b10a8412a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "TEST_MODE = \"inference\"\n",
        "\n",
        "def get_ax(rows=1, cols=1, size=16):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Adjust the size attribute to control how big to render images\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "# Load validation dataset\n",
        "dataset = ShapesDataset()\n",
        "dataset.load_shapes(\"val\")\n",
        "dataset.prepare()\n",
        "\n",
        "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images: 3\n",
            "Classes: ['BG', 'nucleus']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJXL16QLg9Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = os.path.join(MODEL_DIR, \"mask_rcnn.h5\")\n",
        "\n",
        "DEVICE = \"/gpu:0\"\n",
        "with tf.device(DEVICE):\n",
        "    nmodel = modellib.MaskRCNN(mode=\"inference\",\n",
        "                              model_dir=os.getcwd(),\n",
        "                              config=inference_config)\n",
        "    \n",
        "nmodel.load_weights(mask_rcnn_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FYbCRISg9Sy",
        "colab_type": "code",
        "outputId": "0d88a7e1-6df1-43c2-f016-346a5f888398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "image_id = random.choice(dataset.image_ids)\n",
        "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
        "info = dataset.image_info[image_id]\n",
        "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
        "                                       dataset.image_reference(image_id)))\n",
        "print(\"Original image shape: \", modellib.parse_image_meta(image_meta[np.newaxis,...])[\"original_image_shape\"][0])\n",
        "\n",
        "# Run object detection\n",
        "results = nmodel.detect_molded(np.expand_dims(image, 0), np.expand_dims(image_meta, 0), verbose=1)\n",
        "\n",
        "# Display results\n",
        "r = results[0]\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "# Compute AP over range 0.5 to 0.95 and print it\n",
        "utils.compute_ap_range(gt_bbox, gt_class_id, gt_mask,\n",
        "                       r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
        "                       verbose=1)\n",
        "\n",
        "visualize.display_differences(\n",
        "    image,\n",
        "    gt_bbox, gt_class_id, gt_mask,\n",
        "    r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
        "    dataset.class_names, ax=get_ax(),\n",
        "    show_box=False, show_mask=False,\n",
        "    iou_threshold=0.5, score_threshold=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f670569de0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n\u001b[0;32m----> 5\u001b[0;31m                                        dataset.image_reference(image_id)))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original image shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_image_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"original_image_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-70fc55c5ee66>\u001b[0m in \u001b[0;36mimage_reference\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"shapes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shapes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'shapes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL6Xdq-Jg9Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzOQStdg9Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh1ACanvg9c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = os.path.join(MODEL_DIR, \"mask_rcnn.h5\")\n",
        "\n",
        "class ShapesConfig(Config):\n",
        "    \"\"\"Configuration for training on the dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    # Train on 1 GPU and 1 images per GPU. We can put multiple images on each\n",
        "    # GPU. Batch size is (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # background + nucleus\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "\n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 64, 128, 256)  # anchor side in pixels\n",
        "\n",
        "    # Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 800\n",
        "\n",
        "    # set number of epoch\n",
        "    STEPS_PER_EPOCH = 200\n",
        "\n",
        "    # set validation steps \n",
        "    VALIDATION_STEPS = 50\n",
        "\n",
        "class InferenceConfig(ShapesConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "DEVICE = \"/gpu:0\"\n",
        "with tf.device(DEVICE):\n",
        "    nmodel = modellib.MaskRCNN(mode=\"inference\",\n",
        "                              model_dir=os.getcwd(),\n",
        "                              config=inference_config)\n",
        "    \n",
        "nmodel.load_weights(mask_rcnn_path, by_name=True)\n",
        "\n",
        "\n",
        "\n",
        "# Run detection and predict nuclei on a patch\n",
        "results = nmodel.detect([X_test[0]], verbose=1)\n",
        "r = results[0]\n",
        "visualize.display_instances(X_test[0], r['rois'], r['masks'],r['class_ids'],['BG', 'Nuclei'],\n",
        "                      figsize=(16, 16), ax=None,\n",
        "                      show_mask=False, show_bbox=False,\n",
        "                      title=\"Predictions\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eHFxhNuGcbO",
        "colab_type": "code",
        "outputId": "64b72216-3293-4641-91ac-95bb7e007f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "patch_size = 128\n",
        "\n",
        "# Generate an n-ary nuclei mask \n",
        "individual_nuclei = r['masks']\n",
        "predicted_nuclei = np.zeros((patch_size,patch_size), dtype = int)\n",
        "n_nuc = 0\n",
        "for k in range(individual_nuclei.shape[2]):\n",
        "    n_nuc += 1\n",
        "    nuc_mask = r['masks'][:,:,k]\n",
        "    predicted_nuclei += (n_nuc)*nuc_mask\n",
        "plt.imshow(predicted_nuclei)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f50170852e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzP0fC6e+SxYojpMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOSilV__LYTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = r['rois']\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}